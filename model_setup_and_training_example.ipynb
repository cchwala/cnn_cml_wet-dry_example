{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CML Wet/Dry classification with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "1. Load data from NetCDF\n",
    "\n",
    "This step is omitted since the full data set is not available (see README.md). However, there is some dummy code to illustrate how the data set is shaped.\n",
    "\n",
    "2. Build CNN\n",
    "\n",
    "This step is easy using the Keras API.\n",
    "\n",
    "3. Train CNN\n",
    "\n",
    "Also omitted since the training data is missing. The weights of the trained model are in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.utils import shuffle\n",
    "# neural network\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "#import tensorflow for optional direct use\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "random_state = 1234\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is an example of how to use tensorflow to limit the memory that is used on the GPU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Limit GPU memory usage to avoid processes to run out of memory. \n",
    "# For a list of processes blocking GPU memory on an nvidia GPU type 'nvidia-smi' in the terminal.\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "## Keras Metrics ##\n",
    "###################\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Matthews correlation metric.\n",
    "    It is only computed as a batch-wise average, not globally.\n",
    "    Computes the Matthews correlation coefficient measure for quality\n",
    "    of binary classifiers.\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "def tpr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    True positive rate\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(y_pred)\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.round(y_true)\n",
    "    y_neg = 1 - y_pos\n",
    "    tp = K.sum(y_pos * y_pred_pos)/K.sum(y_pos)\n",
    "    return tp\n",
    "\n",
    "def tnr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    True negative rate\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(y_pred)\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.round(y_true)\n",
    "    y_neg = 1 - y_pos\n",
    "    tn = K.sum(y_neg * y_pred_neg)/K.sum(y_neg)\n",
    "    return tn\n",
    "\n",
    "def fpr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    False positive rate\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(y_pred)\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.round(y_true)\n",
    "    y_neg = 1 - y_pos\n",
    "    fp = K.sum(y_neg * y_pred_pos)/K.sum(y_neg)\n",
    "    return fp\n",
    "\n",
    "def fnr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    False negative rate\n",
    "    \"\"\"\n",
    "    y_pred_pos = K.round(y_pred)\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.round(y_true)\n",
    "    y_neg = 1 - y_pos\n",
    "    fn = K.sum(y_pos * y_pred_neg)/K.sum(y_pos)\n",
    "    return fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the input data (X_train and X_test) is (number of samples, window size in minutes (180), number of channels(2)). The shape of the binary reference data (y_train and y_test) is (number of samples)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# read training data from e.g. NetCDF\n",
    "X_train, y_train = data_loader_dummy(filename)\n",
    "# shuffle training samples\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# read test data from e.g. NetCDF\n",
    "X_test, y_test = data_loader_dummy(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all parameters, that we have to specify for the model architecture.\n",
    "window = 180\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "dropout = 0.4\n",
    "\n",
    "n_fc_neurons = 64\n",
    "\n",
    "n_filters = [24, 48, 48, 96, 192]\n",
    "\n",
    "batchsize = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 180, 2)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 180, 24)           168       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 180, 24)           1752      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 60, 24)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 60, 48)            3504      \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 60, 48)            6960      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 20, 48)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 20, 48)            6960      \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 20, 48)            6960      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 6, 48)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 6, 96)             13920     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 6, 96)             27744     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 2, 96)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 2, 192)            55488     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                12352     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 140,033\n",
      "Trainable params: 140,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "# Convolutional part\n",
    "input1 = Input(shape=(window,2,))\n",
    "###\n",
    "x1 =Conv1D(filters=n_filters[0], kernel_size=kernel_size, padding='same', activation='relu')(input1)\n",
    "x1 = Conv1D(filters=n_filters[0], kernel_size=kernel_size, padding='same', activation='relu')(x1)\n",
    "x1 = MaxPooling1D(pool_size=kernel_size)(x1)\n",
    "###\n",
    "x1 = Conv1D(filters=n_filters[1], kernel_size=kernel_size, padding='same', activation='relu')(x1)\n",
    "x1 = Conv1D(filters=n_filters[1], kernel_size=kernel_size, padding='same', activation='relu')(x1)\n",
    "x1 = MaxPooling1D(pool_size=kernel_size)(x1)\n",
    "###\n",
    "x1 = Conv1D(filters=n_filters[2], kernel_size=kernel_size, padding='same', activation='relu')(x1)\n",
    "x1 = Conv1D(filters=n_filters[2], kernel_size=kernel_size, padding='same', activation='relu')(x1)\n",
    "x1 = MaxPooling1D(pool_size=kernel_size)(x1)\n",
    "###\n",
    "x1 = Conv1D(filters=n_filters[3], kernel_size=kernel_size, padding='same', activation='relu')(x1)\n",
    "x1 = Conv1D(filters=n_filters[3], kernel_size=kernel_size, padding='same', activation='relu')(x1)\n",
    "x1 = MaxPooling1D(pool_size=kernel_size)(x1)\n",
    "###\n",
    "x1 = Conv1D(filters=n_filters[4], kernel_size=kernel_size, padding='same', activation='relu')(x1)\n",
    "x1 = GlobalAveragePooling1D()(x1)\n",
    "# FC part\n",
    "x1 = Dense(n_fc_neurons, activation='relu')(x1)\n",
    "x1 = Dropout(dropout)(x1)\n",
    "x1 = Dense(n_fc_neurons, activation='relu')(x1)\n",
    "x1 = Dropout(dropout)(x1)\n",
    "out = Dense(1, activation='sigmoid')(x1)\n",
    "\n",
    "model = keras.models.Model(inputs=input1, outputs=out)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model needs to be compiled first\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=SGD(lr=0.008, decay =1e-3, momentum=0.9, nesterov=True),\n",
    "             metrics=['accuracy', 'mse', matthews_correlation , tpr, tnr, fpr, fnr]\n",
    "             )\n",
    "# optional callbacks for the training\n",
    "callbacks_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of training samples is reduced to a multiple of the batch size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clipping = int(len(X_train)/batchsize)*batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "history = model.fit(X_train[:clipping], y_train[:clipping], \n",
    "          epochs=2000, \n",
    "          callbacks=callbacks_list, \n",
    "          batch_size = batchsize, \n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model, weights and training history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serialize model to JSON"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model_v5.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serialize weights to HDF5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save_weights(\"model_v5.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save training history"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('training_history_model_v5.json', 'w') as f:\n",
    "    json.dump(history.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
